{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice: Grupos de dispositivos móviles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"file:/home/clement/Desktop/formacion-hadoop/Ejercicios MLLIB - ML-20200114/datasets y soluciones/ml/devicestatus.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile(filename).map(lambda x: x.split(\"|\"))\\\n",
    "                            .map(lambda x: (float(x[12]), float(x[13])))\\\n",
    "                            .filter(lambda x: x != (0,0))\\\n",
    "                            .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33.6894754264, -117.543308253),\n",
       " (37.4321088904, -121.485029632),\n",
       " (39.4378908349, -120.938978486),\n",
       " (39.3635186767, -119.400334708),\n",
       " (33.1913581092, -116.448242643)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "431857"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans.train(data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  34.29049653, -117.77978731]),\n",
       " array([  45.33695551, -120.99431457]),\n",
       " array([  35.08592001, -112.57643827]),\n",
       " array([  37.96974389, -121.20684208]),\n",
       " array([  41.97751673, -121.58231811])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict((33, -116))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(point): \n",
    "    center = model.centers[model.predict(point)] \n",
    "    return np.sqrt(sum([x**2 for x in (point - center)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Error = 607334.5450057113\n"
     ]
    }
   ],
   "source": [
    "WSSSE = data.map(lambda point: error(point)).reduce(lambda x, y: x + y) \n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice: Recomendación de películas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Películas recomendadas para tí:\n",
      "Song of Freedom (1936)\n",
      "Neon Bible, The (1995)\n",
      "Cotton Mary (1999)\n",
      "Mamma Roma (1962)\n",
      "Chain of Fools (2000)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark.mllib.recommendation import Rating\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "ratings_filename = \"file:///home/clement/Desktop/formacion-hadoop/Ejercicios MLLIB - ML-20200114/datasets y soluciones/ml/als/ratings.dat\"\n",
    "personalRatings_filename = \"file:///home/clement/Desktop/formacion-hadoop/Ejercicios MLLIB - ML-20200114/datasets y soluciones/ml/als/personalRatings.txt\"\n",
    "movies_filename = \"file:///home/clement/Desktop/formacion-hadoop/Ejercicios MLLIB - ML-20200114/datasets y soluciones/ml/als/movies.dat\"\n",
    "\n",
    "def parseRating(line):\n",
    "    # Realizar el procesamiento de la línea para devolver únicamente los datos necesarios\n",
    "    fields = line.split(\"::\")\n",
    "    return int(fields[0]), int(fields[1]), float(fields[2])\n",
    "\n",
    "def parseMovie(line):\n",
    "    # Realizar el procesamiento de la línea para devolver únicamente los datos necesarios\n",
    "    fields = line.split(\"::\")\n",
    "    return (int(fields[0]), fields[1])\n",
    "\n",
    "movies = sc.textFile(movies_filename).map(lambda x: parseMovie(x))\n",
    "#Lee y crea el Pair RDD de películas obteniendo en cada registro únicamente los datos necesarios (idpelícula, nombre)\n",
    "\n",
    "ratings = sc.textFile(ratings_filename+\",\"+personalRatings_filename).map(lambda x: parseRating(x))\n",
    "#Lee y crea RDD de valoraciones obteniendo en cada registro únicamente los datos necesarios (idusuario,idpelicula, valoración)\n",
    "\n",
    "ratingsR = ratings.map(lambda x: Rating(x[0], x[1], x[2])) \n",
    "#Crea el RDD de entrada al algoritmo con los registros de tipo Rating\n",
    "model = ALS.train(ratingsR, rank=2)\n",
    "#Llama al modelo de entrenamiento ALS\n",
    "result = model.recommendProducts(0, 5)\n",
    "#Obtén las 5 recomendaciones para nuestro usuario (id=0)\n",
    "\n",
    "resultRDD = sc.parallelize(result)\n",
    "\n",
    "resultJ = resultRDD.map(lambda x: (x[1], x[2]))\n",
    "#Genera un nuevo Pair RDD con los resultados obtenidos para poder realizar posteriormente un JOIN con el RDD de películas\n",
    "joinMovies = resultJ.join(movies)\n",
    "#Realiza el join con el RDD anterior y el Pair RDD de películas creado al inicio\n",
    "\n",
    "print(\"Películas recomendadas para tí:\")\n",
    "for i in joinMovies.collect():\n",
    "    print(i[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 0.7505228457756725\n"
     ]
    }
   ],
   "source": [
    "testdata = ratings.map(lambda p: (p[0], p[1]))\n",
    "predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPreds = ratings.map(lambda r: ((int(r[0]), int(r[1])), int(r[2]))).join(predictions)\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice: Análisis de sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valoraciones_filename = \"file:///home/clement/Desktop/formacion-hadoop/Ejercicios MLLIB - ML-20200114/datasets y soluciones/ml/sentimiento.txt\"\n",
    "valoraciones = sc.textFile(valoraciones_filename).map(lambda x: x.replace(\"neg\",\"0\").replace(\"pos\",\"1\")).map(lambda x: x.split(\";\")).map(lambda x: Row(x[0], float(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Row('a�icos', 0.0)>,\n",
       " <Row('abandonada', 0.0)>,\n",
       " <Row('abandonadas', 0.0)>,\n",
       " <Row('abandonado', 0.0)>,\n",
       " <Row('abandonados', 0.0)>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valoraciones.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "valoracionesDF = valoraciones.toDF([\"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       text|label|\n",
      "+-----------+-----+\n",
      "|     a�icos|  0.0|\n",
      "| abandonada|  0.0|\n",
      "|abandonadas|  0.0|\n",
      "| abandonado|  0.0|\n",
      "|abandonados|  0.0|\n",
      "|  abandonar|  0.0|\n",
      "|  abandonos|  0.0|\n",
      "| abarrotada|  0.0|\n",
      "|abarrotadas|  0.0|\n",
      "| abarrotado|  0.0|\n",
      "|abarrotados|  0.0|\n",
      "|    abatida|  0.0|\n",
      "|   abatidas|  0.0|\n",
      "|    abatido|  0.0|\n",
      "|   abatidos|  0.0|\n",
      "|abatimiento|  0.0|\n",
      "|     abatir|  0.0|\n",
      "| abigarrada|  0.0|\n",
      "|abigarradas|  0.0|\n",
      "| abigarrado|  0.0|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valoracionesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------------+\n",
      "|       text|label|        words|\n",
      "+-----------+-----+-------------+\n",
      "|     a�icos|  0.0|     [a�icos]|\n",
      "| abandonada|  0.0| [abandonada]|\n",
      "|abandonadas|  0.0|[abandonadas]|\n",
      "| abandonado|  0.0| [abandonado]|\n",
      "|abandonados|  0.0|[abandonados]|\n",
      "+-----------+-----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tokenized = tokenizer.transform(valoracionesDF)\n",
    "tokenized.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------------+--------------------+\n",
      "|       text|label|        words|            features|\n",
      "+-----------+-----+-------------+--------------------+\n",
      "|     a�icos|  0.0|     [a�icos]|(262144,[83924],[...|\n",
      "| abandonada|  0.0| [abandonada]|(262144,[246055],...|\n",
      "|abandonadas|  0.0|[abandonadas]|(262144,[204805],...|\n",
      "| abandonado|  0.0| [abandonado]|(262144,[125409],...|\n",
      "|abandonados|  0.0|[abandonados]|(262144,[201041],...|\n",
      "+-----------+-----+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "hashedTF = hashingTF.transform(tokenized)\n",
    "hashedTF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "# Entrenamos el pipeline sobre los datos de entrenamiento\n",
    "model = pipeline.fit(valoracionesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sqlContext.createDataFrame([\n",
    "(4,\"En el restaurante Ginos hacen buenos platos\"),\n",
    "(5,\"Pobres indefensos animales\"),\n",
    "(6,\"Me pedi una pizza en el telepizza y estaba fria\"),\n",
    "(7,\"Estoy muy motivado gracias a este curso\")], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, text='En el restaurante Ginos hacen buenos platos', prediction=1.0)\n",
      "Row(id=5, text='Pobres indefensos animales', prediction=0.0)\n",
      "Row(id=6, text='Me pedi una pizza en el telepizza y estaba fria', prediction=0.0)\n",
      "Row(id=7, text='Estoy muy motivado gracias a este curso', prediction=1.0)\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
