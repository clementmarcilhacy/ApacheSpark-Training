{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"formacion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice: Comenzando con los RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = sc.textFile(\"file:/home/clement/Desktop/formacion-hadoop/spark-datasets-soluciones/datasets/weblogs/*.log\")\n",
    "cuentas = sc.textFile(\"file:/home/clement/Desktop/formacion-hadoop/spark-datasets-soluciones/datasets/accounts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs1 = logs.map(lambda x: (x.split()[2], 1)).reduceByKey(lambda v1, v2: v1 + v2).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('64', 562), ('25079', 26), ('63453', 18), ('17444', 30), ('26363', 6)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuentas1 = cuentas.keyBy(lambda x: x.split(\",\")[0]).mapValues(lambda x: list(x.split(\",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1',\n",
       "  ['1',\n",
       "   '2008-12-31 15:05:45',\n",
       "   '2013-12-29 09:53:35',\n",
       "   'Donald',\n",
       "   'Becton',\n",
       "   '2275 Washburn Street',\n",
       "   'Oakland',\n",
       "   'CA',\n",
       "   '94656',\n",
       "   '5104529635',\n",
       "   '2013-12-27 15:01:36',\n",
       "   '2013-12-27 15:01:36']),\n",
       " ('2',\n",
       "  ['2',\n",
       "   '2008-10-31 01:29:51',\n",
       "   '\\\\N',\n",
       "   'Donna',\n",
       "   'Jones',\n",
       "   '3885 Elliott Street',\n",
       "   'Santa Rosa',\n",
       "   'CA',\n",
       "   '94978',\n",
       "   '7072247159',\n",
       "   '2013-12-27 15:01:36',\n",
       "   '2013-12-27 15:01:36']),\n",
       " ('3',\n",
       "  ['3',\n",
       "   '2008-12-14 00:49:06',\n",
       "   '\\\\N',\n",
       "   'Dorthy',\n",
       "   'Chalmers',\n",
       "   '4073 Whaley Lane',\n",
       "   'Santa Rosa',\n",
       "   'CA',\n",
       "   '94922',\n",
       "   '7079781240',\n",
       "   '2013-12-27 15:01:36',\n",
       "   '2013-12-27 15:01:36']),\n",
       " ('4',\n",
       "  ['4',\n",
       "   '2008-12-13 02:07:05',\n",
       "   '2014-01-10 21:53:21',\n",
       "   'Leila',\n",
       "   'Spencer',\n",
       "   '1447 Ross Street',\n",
       "   'San Francisco',\n",
       "   'CA',\n",
       "   '94041',\n",
       "   '4158217430',\n",
       "   '2013-12-27 15:01:36',\n",
       "   '2013-12-27 15:01:36']),\n",
       " ('5',\n",
       "  ['5',\n",
       "   '2008-11-30 02:25:21',\n",
       "   '\\\\N',\n",
       "   'Anita',\n",
       "   'Laughlin',\n",
       "   '2767 Hill Street',\n",
       "   'Palo Alto',\n",
       "   'CA',\n",
       "   '94318',\n",
       "   '6502067779',\n",
       "   '2013-12-27 15:01:36',\n",
       "   '2013-12-27 15:01:36'])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuentas1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined2 = cuentas1.join(logs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('77',\n",
       "  (['77',\n",
       "    '2008-11-27 00:38:05',\n",
       "    '\\\\N',\n",
       "    'Doris',\n",
       "    'Miller',\n",
       "    '2928 Bel Meadow Drive',\n",
       "    'Berkeley',\n",
       "    'CA',\n",
       "    '94769',\n",
       "    '5102555518',\n",
       "    '2013-12-27 15:01:36',\n",
       "    '2013-12-27 15:01:36'],\n",
       "   603)),\n",
       " ('748',\n",
       "  (['748',\n",
       "    '2009-03-03 10:18:25',\n",
       "    '\\\\N',\n",
       "    'Jason',\n",
       "    'Oneal',\n",
       "    '381 Evergreen Lane',\n",
       "    'San Francisco',\n",
       "    'CA',\n",
       "    '94129',\n",
       "    '4159356276',\n",
       "    '2013-12-27 15:01:38',\n",
       "    '2013-12-27 15:01:38'],\n",
       "   22)),\n",
       " ('1068',\n",
       "  (['1068',\n",
       "    '2009-07-09 07:14:04',\n",
       "    '2014-02-06 05:08:44',\n",
       "    'Pat',\n",
       "    'Armstrong',\n",
       "    '966 Leisure Lane',\n",
       "    'Sacramento',\n",
       "    'CA',\n",
       "    '95653',\n",
       "    '9161449236',\n",
       "    '2013-12-27 15:01:38',\n",
       "    '2013-12-27 15:01:38'],\n",
       "   18)),\n",
       " ('1317',\n",
       "  (['1317',\n",
       "    '2009-03-05 07:49:21',\n",
       "    '\\\\N',\n",
       "    'Kent',\n",
       "    'Harrison',\n",
       "    '3098 Henry Ford Avenue',\n",
       "    'Sacramento',\n",
       "    'CA',\n",
       "    '95724',\n",
       "    '9162447923',\n",
       "    '2013-12-27 15:01:39',\n",
       "    '2013-12-27 15:01:39'],\n",
       "   24)),\n",
       " ('1970',\n",
       "  (['1970',\n",
       "    '2009-03-29 04:11:40',\n",
       "    '2010-02-11 05:14:55',\n",
       "    'James',\n",
       "    'Harris',\n",
       "    '2672 Maple Street',\n",
       "    'Stockton',\n",
       "    'CA',\n",
       "    '95385',\n",
       "    '2090364271',\n",
       "    '2013-12-27 15:01:40',\n",
       "    '2013-12-27 15:01:40'],\n",
       "   30))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 603 Doris Miller\n",
      "748 22 Jason Oneal\n",
      "1068 18 Pat Armstrong\n",
      "1317 24 Kent Harrison\n",
      "1970 30 James Harris\n"
     ]
    }
   ],
   "source": [
    "for elmt in joined2.take(5):\n",
    "    print(elmt[0], elmt[1][1], elmt[1][0][3], elmt[1][0][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('94656',\n",
       "  '1,2008-12-31 15:05:45,2013-12-29 09:53:35,Donald,Becton,2275 Washburn Street,Oakland,CA,94656,5104529635,2013-12-27 15:01:36,2013-12-27 15:01:36'),\n",
       " ('94978',\n",
       "  '2,2008-10-31 01:29:51,\\\\N,Donna,Jones,3885 Elliott Street,Santa Rosa,CA,94978,7072247159,2013-12-27 15:01:36,2013-12-27 15:01:36'),\n",
       " ('94922',\n",
       "  '3,2008-12-14 00:49:06,\\\\N,Dorthy,Chalmers,4073 Whaley Lane,Santa Rosa,CA,94922,7079781240,2013-12-27 15:01:36,2013-12-27 15:01:36'),\n",
       " ('94041',\n",
       "  '4,2008-12-13 02:07:05,2014-01-10 21:53:21,Leila,Spencer,1447 Ross Street,San Francisco,CA,94041,4158217430,2013-12-27 15:01:36,2013-12-27 15:01:36'),\n",
       " ('94318',\n",
       "  '5,2008-11-30 02:25:21,\\\\N,Anita,Laughlin,2767 Hill Street,Palo Alto,CA,94318,6502067779,2013-12-27 15:01:36,2013-12-27 15:01:36')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuentas_cp = cuentas.keyBy(lambda x: x.split(\",\")[8])\n",
    "cuentas_cp.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_tuples(x, y):\n",
    "    res = []\n",
    "    if isinstance(x, tuple) and isinstance(y, tuple):\n",
    "        res.append(x)\n",
    "        res.append(y)\n",
    "    elif isinstance(x, tuple):\n",
    "        for elmt in y:\n",
    "            res.append(elmt)\n",
    "        res.append(x)\n",
    "    elif isinstance(y, tuple):\n",
    "        for elmt in x:\n",
    "            res.append(elmt)\n",
    "        res.append(y)\n",
    "    else:\n",
    "        for elmt in x:\n",
    "            res.append(elmt)\n",
    "        for elmt in y:\n",
    "            res.append(elmt)\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('85000',\n",
       "  [('Leon', 'Willson'),\n",
       "   ('Ronald', 'Clark'),\n",
       "   ('Juanita', 'Rush'),\n",
       "   ('Roger', 'Woodhouse'),\n",
       "   ('Colin', 'Baptist'),\n",
       "   ('Percy', 'King'),\n",
       "   ('David', 'Carmack'),\n",
       "   ('Ana', 'Milan'),\n",
       "   ('Kendra', 'McCurdy'),\n",
       "   ('Robert', 'Pitts'),\n",
       "   ('Leslie', 'Hopkins'),\n",
       "   ('Paul', 'Butler'),\n",
       "   ('Phyllis', 'Barth')]),\n",
       " ('85001',\n",
       "  [('David', 'Cross'),\n",
       "   ('Danny', 'Pritchett'),\n",
       "   ('Lennie', 'Sistrunk'),\n",
       "   ('Jeffery', 'Sweet'),\n",
       "   ('Nancy', 'Buckles'),\n",
       "   ('Katie', 'James'),\n",
       "   ('Anthony', 'Tutor'),\n",
       "   ('Mark', 'Battle'),\n",
       "   ('Carol', 'Hiller'),\n",
       "   ('Nancy', 'Landa'),\n",
       "   ('Sergio', 'Marks'),\n",
       "   ('Barbara', 'Sprague'),\n",
       "   ('Angela', 'Greenwell'),\n",
       "   ('Sabrina', 'Helms'),\n",
       "   ('Sharon', 'Allen'),\n",
       "   ('Gregory', 'Waller')]),\n",
       " ('85002',\n",
       "  [('Alan', 'Whitmore'),\n",
       "   ('Tara', 'Chandler'),\n",
       "   ('Diane', 'Robinson'),\n",
       "   ('Henry', 'Brown'),\n",
       "   ('Lacey', 'Sisson'),\n",
       "   ('Elfriede', 'Root'),\n",
       "   ('Barbara', 'Lynch'),\n",
       "   ('Jeremy', 'Dixon'),\n",
       "   ('David', 'Hampton'),\n",
       "   ('Elizabeth', 'Norman'),\n",
       "   ('Lynnette', 'Granados'),\n",
       "   ('Martha', 'Sullivan'),\n",
       "   ('Grant', 'Novak'),\n",
       "   ('Katrina', 'Johnson'),\n",
       "   ('Jennifer', 'McConville'),\n",
       "   ('Rose', 'Sherer')]),\n",
       " ('85003',\n",
       "  [('Thad', 'Jenkins'),\n",
       "   ('Edward', 'Rick'),\n",
       "   ('Ivy', 'Lindsay'),\n",
       "   ('Beth', 'Oneil'),\n",
       "   ('Elizabeth', 'Post'),\n",
       "   ('Maude', 'Taylor'),\n",
       "   ('Wesley', 'Smith'),\n",
       "   ('Gilbert', 'Wolfe'),\n",
       "   ('Richard', 'Jenkins'),\n",
       "   ('David', 'Carmichael'),\n",
       "   ('Earl', 'Jaimes'),\n",
       "   ('Guillermo', 'Jennings')]),\n",
       " ('85004',\n",
       "  [('Eric', 'Morris'),\n",
       "   ('Hazel', 'Reiser'),\n",
       "   ('Alicia', 'Gregg'),\n",
       "   ('Elizabeth', 'Preston'),\n",
       "   ('Julie', 'Hass'),\n",
       "   ('Debra', 'Gunn'),\n",
       "   ('Terry', 'Frye'),\n",
       "   ('Jack', 'Reyes'),\n",
       "   ('Brenda', 'Thomas'),\n",
       "   ('Lloyd', 'Stowe'),\n",
       "   ('Tommy', 'Vaughn'),\n",
       "   ('Barry', 'Edmond')])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuentas_cp1 = cuentas_cp.mapValues(lambda x: (x.split(\",\")[3], x.split(\",\")[4])).reduceByKey(lambda x,y: concat_tuples(x,y))\n",
    "cuentas_cp1.sortByKey().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice: Trabajando con particiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load XML files containing device activation records.\n",
    "# Find the most common device models activated\n",
    "\n",
    "import xml.etree.ElementTree as ElementTree\n",
    "\n",
    "# Given a partition containing multi-line XML, parse the contents. \n",
    "# Return an iterator of activation Elements contained in the partition\n",
    "def getactivations(fileiterator):\n",
    "    s = ''\n",
    "    for i in fileiterator: s = s + str(i)\n",
    "    filetree = ElementTree.fromstring(s)\n",
    "    return filetree.getiterator('activation')\n",
    "\n",
    "# Get the model name from a device activation record\n",
    "def getmodel(activation):\n",
    "    return activation.find('model').text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = sc.textFile(\"/tmp/curso/activations/*.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method RDD.toDebugString of /tmp/curso/activations/*.xml MapPartitionsRDD[30] at textFile at NativeMethodAccessorImpl.java:0>\n"
     ]
    }
   ],
   "source": [
    "print(activations.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<activations>',\n",
       " '\\t  <activation timestamp=\"1357019215\" type=\"phone\">',\n",
       " '\\t    <account-number>55800</account-number>',\n",
       " '\\t    <device-id>4724bb2e-22fc-42b4-a1b2-17d7f2698679</device-id>',\n",
       " '\\t    <phone-number>6262974993</phone-number>',\n",
       " '\\t    <model>MeeToo 3.0</model>',\n",
       " '\\t  </activation>',\n",
       " '\\t  \\t\\t  <activation timestamp=\"1357019147\" type=\"phone\">',\n",
       " '\\t    <account-number>57735</account-number>',\n",
       " '\\t    <device-id>8f38bab2-ad75-42ab-b918-0f16bb95c587</device-id>']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element 'activation' at 0x7f957c2214d0>,\n",
       " <Element 'activation' at 0x7f957c1ba7d0>,\n",
       " <Element 'activation' at 0x7f957c1ba290>,\n",
       " <Element 'activation' at 0x7f957c1f2dd0>,\n",
       " <Element 'activation' at 0x7f957c1f2950>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations1 = activations.mapPartitions(lambda x: getactivations(x))\n",
    "activations1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MeeToo 3.0', 'MeeToo 4.1', 'Sorrento F11L', 'Sorrento F22L', 'Sorrento F22L']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = activations1.map(lambda x: getmodel(x))\n",
    "models.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method RDD.toDebugString of PythonRDD[34] at RDD at PythonRDD.scala:53>\n"
     ]
    }
   ],
   "source": [
    "print(models.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "models1 = models.map(lambda x: (x, 1)).reduceByKey(lambda v1, v2: v1 + v2)#.sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('iFruit 2', 4061),\n",
       " ('Ronin S1', 4027),\n",
       " ('Titanic 4000', 1378),\n",
       " ('Sorrento F23L', 2676),\n",
       " ('Sorrento F10L', 4633)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "models2 = models1.map(lambda x: (x[1], x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5301, 'Titanic 1100'),\n",
       " (5238, 'Sorrento F00L'),\n",
       " (5232, 'Titanic 1000'),\n",
       " (5144, 'iFruit 1'),\n",
       " (5019, 'MeeToo 1.0'),\n",
       " (4863, 'Sorrento F01L'),\n",
       " (4633, 'Sorrento F10L'),\n",
       " (4504, 'Titanic 2000'),\n",
       " (4159, 'Titanic 2100'),\n",
       " (4132, 'Titanic 2200')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models2.top(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice: Usando cache con los RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.7 ms, sys: 4.84 ms, total: 19.6 ms\n",
      "Wall time: 3.91 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "models2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.97 ms, sys: 637 µs, total: 5.61 ms\n",
      "Wall time: 28.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[43] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "models2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.9 ms, sys: 4.89 ms, total: 19.8 ms\n",
      "Wall time: 7.38 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "models2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.87 ms, sys: 4.64 ms, total: 14.5 ms\n",
      "Wall time: 3.68 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "models2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice: Usando checkpoints con los RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir(\"checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = sc.parallelize([i for i in range(1,6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    if (i+1)%10 == 0:\n",
    "        my_data.checkpoint()\n",
    "    my_data = my_data.map(lambda x: x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "for x in my_data.collect():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(1) PythonRDD[69] at collect at <ipython-input-35-9c1275e03a17>:1 []\\n |  PythonRDD[66] at RDD at PythonRDD.scala:53 []\\n |  ReliableCheckpointRDD[68] at count at <ipython-input-34-56111a71605a>:1 []'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice: Escribiendo y ejecutando una aplicación Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poner en el escript de python al principio sc = pyspark.SparkContext() y ejecutarlo en el commandline.\n",
    "\n",
    "Ex: spark-submit script.py /tmp/curso/weblogs/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice: Usando variables de broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = sc.textFile(\"/tmp/curso/weblogs/*.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"/home/clement/Desktop/formacion-hadoop/spark-datasets-soluciones/datasets/targetmodels.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "with open(\"/home/clement/Desktop/formacion-hadoop/spark-datasets-soluciones/datasets/targetmodels.txt\") as f:\n",
    "    models = f.readlines()\n",
    "models = [x.strip() for x in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iFruit 5A',\n",
       " 'iFruit 5',\n",
       " 'iFruit 4A',\n",
       " 'iFruit 4',\n",
       " 'Titanic 4000',\n",
       " 'Titanic 3000',\n",
       " 'Titanic 2500',\n",
       " 'Sorrento F41L',\n",
       " 'Sorrento F40L',\n",
       " 'Ronin S4',\n",
       " 'Ronin S3',\n",
       " 'Ronin S2',\n",
       " 'MeeToo 5.1',\n",
       " 'MeeToo 5.0']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_bc = sc.broadcast(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_filtered = logs.filter(lambda x: any(mod in x for mod in models_bc.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['208.78.30.239 - 15196 [21/Jan/2014:23:57:31 +0100] \"GET /KBDOC-00171.html HTTP/1.0\" 200 11814 \"http://www.loudacre.com\"  \"Loudacre Mobile Browser Sorrento F41L\"',\n",
       " '208.78.30.239 - 15196 [21/Jan/2014:23:57:31 +0100] \"GET /theme.css HTTP/1.0\" 200 19080 \"http://www.loudacre.com\"  \"Loudacre Mobile Browser Sorrento F41L\"',\n",
       " '41.112.222.164 - 104281 [21/Jan/2014:23:55:54 +0100] \"GET /KBDOC-00206.html HTTP/1.0\" 200 8961 \"http://www.loudacre.com\"  \"Loudacre Mobile Browser MeeToo 5.0\"',\n",
       " '41.112.222.164 - 104281 [21/Jan/2014:23:55:54 +0100] \"GET /theme.css HTTP/1.0\" 200 6082 \"http://www.loudacre.com\"  \"Loudacre Mobile Browser MeeToo 5.0\"',\n",
       " '167.68.23.7 - 13926 [21/Jan/2014:23:47:23 +0100] \"GET /KBDOC-00288.html HTTP/1.0\" 200 15636 \"http://www.loudacre.com\"  \"Loudacre Mobile Browser Sorrento F41L\"']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_filtered.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55348"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice: Usando acumuladores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_acc = sc.accumulator(0)\n",
    "css_acc = sc.accumulator(0)\n",
    "jpg_acc = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = sc.textFile(\"/tmp/curso/weblogs/*.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar(line, html_lines, css_lines, jpg_lines):\n",
    "    if \".html\" in line:\n",
    "        html_lines += 1\n",
    "    if \".css\" in line:\n",
    "        css_lines += 1\n",
    "    if \".jpg\" in line:\n",
    "        jpg_lines += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs.foreach(lambda x: contar(x, html_acc, css_acc, jpg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182067\n"
     ]
    }
   ],
   "source": [
    "print(html_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182067\n"
     ]
    }
   ],
   "source": [
    "print(css_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24743\n"
     ]
    }
   ],
   "source": [
    "print(jpg_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice: Explorando Spark streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it work, first start the netcat server in a terminal with \"nc -lk 4444\" and THEN execute the cell/run the shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o909.start.\n: java.lang.IllegalStateException: Only one StreamingContext may be started in this JVM. Currently running StreamingContext was started atorg.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:557)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:566)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.base/java.lang.Thread.run(Thread.java:834)\n\tat org.apache.spark.streaming.StreamingContext$.org$apache$spark$streaming$StreamingContext$$assertNoOtherContextIsActive(StreamingContext.scala:762)\n\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:575)\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:557)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-821aad54c8d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mwordCounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# Start the computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait for the computation to terminate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mStart\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mexecution\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mStreamingContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activeContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o909.start.\n: java.lang.IllegalStateException: Only one StreamingContext may be started in this JVM. Currently running StreamingContext was started atorg.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:557)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:566)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.base/java.lang.Thread.run(Thread.java:834)\n\tat org.apache.spark.streaming.StreamingContext$.org$apache$spark$streaming$StreamingContext$$assertNoOtherContextIsActive(StreamingContext.scala:762)\n\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:575)\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:557)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# Create a DStream that will connect to hostname:port, like localhost:4444\n",
    "lines = ssc.socketTextStream(\"localhost\", 4444)\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "wordCounts.pprint()\n",
    "\n",
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Same with updateStateBykey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=NetworkWordCount, master=local[2]) created by __init__ at <ipython-input-64-821aad54c8d8>:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-dbcd9ad0537e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create a local StreamingContext with two working thread and batch interval of 1 second\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[2]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NetworkWordCount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mssc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStreamingContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    333\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 335\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    336\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=NetworkWordCount, master=local[2]) created by __init__ at <ipython-input-64-821aad54c8d8>:5 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# Create a DStream that will connect to hostname:port, like localhost:4444\n",
    "lines = ssc.socketTextStream(\"localhost\", 4444)\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "ssc.checkpoint(\"checkpoints\")\n",
    "\n",
    "def updateCount(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)\n",
    "\n",
    "totalWordCounts = wordCounts.updateStateByKey(updateCount)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "totalWordCounts.pprint()\n",
    "\n",
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Directly in the spark shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run the spark shell with 2 threads context:\n",
    "- \"pyspark --master local[2]\"\n",
    "\n",
    "The in the spark shell execute the next commands:\n",
    "- ssc = StreamingContext(sc, 5)\n",
    "- lines = ssc.socketTextStream(\"localhost\", 4444)\n",
    "- words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "- pairs = words.map(lambda word: (word, 1))\n",
    "- wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "- wordCounts.pprint()\n",
    "- ssc.start()             # Start the computation\n",
    "- ssc.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice: Usando Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips = sqlContext.read.json(\"file:/home/clement/Desktop/formacion-hadoop/spark-datasets-soluciones/datasets/zips.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+--------------------+-----+-----+\n",
      "|  _id|           city|                 loc|  pop|state|\n",
      "+-----+---------------+--------------------+-----+-----+\n",
      "|01001|         AGAWAM|[-72.622739, 42.0...|15338|   MA|\n",
      "|01002|        CUSHMAN|[-72.51565, 42.37...|36963|   MA|\n",
      "|01005|          BARRE|[-72.108354, 42.4...| 4546|   MA|\n",
      "|01007|    BELCHERTOWN|[-72.410953, 42.2...|10579|   MA|\n",
      "|01008|      BLANDFORD|[-72.936114, 42.1...| 1240|   MA|\n",
      "|01010|      BRIMFIELD|[-72.188455, 42.1...| 3706|   MA|\n",
      "|01011|        CHESTER|[-72.988761, 42.2...| 1688|   MA|\n",
      "|01012|   CHESTERFIELD|[-72.833309, 42.3...|  177|   MA|\n",
      "|01013|       CHICOPEE|[-72.607962, 42.1...|23396|   MA|\n",
      "|01020|       CHICOPEE|[-72.576142, 42.1...|31495|   MA|\n",
      "|01022|   WESTOVER AFB|[-72.558657, 42.1...| 1764|   MA|\n",
      "|01026|     CUMMINGTON|[-72.905767, 42.4...| 1484|   MA|\n",
      "|01027|      MOUNT TOM|[-72.679921, 42.2...|16864|   MA|\n",
      "|01028|EAST LONGMEADOW|[-72.505565, 42.0...|13367|   MA|\n",
      "|01030|  FEEDING HILLS|[-72.675077, 42.0...|11985|   MA|\n",
      "|01031|   GILBERTVILLE|[-72.198585, 42.3...| 2385|   MA|\n",
      "|01032|         GOSHEN|[-72.844092, 42.4...|  122|   MA|\n",
      "|01033|         GRANBY|[-72.520001, 42.2...| 5526|   MA|\n",
      "|01034|        TOLLAND|[-72.908793, 42.0...| 1652|   MA|\n",
      "|01035|         HADLEY|[-72.571499, 42.3...| 4231|   MA|\n",
      "+-----+---------------+--------------------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zips.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips.createOrReplaceTempView(\"zips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtén los códigos postales con más de 10000 de población"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+--------------------+-----+-----+\n",
      "|  _id|            city|                 loc|  pop|state|\n",
      "+-----+----------------+--------------------+-----+-----+\n",
      "|01001|          AGAWAM|[-72.622739, 42.0...|15338|   MA|\n",
      "|01002|         CUSHMAN|[-72.51565, 42.37...|36963|   MA|\n",
      "|01007|     BELCHERTOWN|[-72.410953, 42.2...|10579|   MA|\n",
      "|01013|        CHICOPEE|[-72.607962, 42.1...|23396|   MA|\n",
      "|01020|        CHICOPEE|[-72.576142, 42.1...|31495|   MA|\n",
      "|01027|       MOUNT TOM|[-72.679921, 42.2...|16864|   MA|\n",
      "|01028| EAST LONGMEADOW|[-72.505565, 42.0...|13367|   MA|\n",
      "|01030|   FEEDING HILLS|[-72.675077, 42.0...|11985|   MA|\n",
      "|01040|         HOLYOKE|[-72.626193, 42.2...|43704|   MA|\n",
      "|01056|          LUDLOW|[-72.471012, 42.1...|18820|   MA|\n",
      "|01060|        FLORENCE|[-72.654245, 42.3...|27939|   MA|\n",
      "|01075|    SOUTH HADLEY|[-72.581137, 42.2...|16699|   MA|\n",
      "|01085|      MONTGOMERY|[-72.754318, 42.1...|40117|   MA|\n",
      "|01089|WEST SPRINGFIELD|[-72.641109, 42.1...|27537|   MA|\n",
      "|01095|       WILBRAHAM|[-72.446415, 42.1...|12635|   MA|\n",
      "|01104|     SPRINGFIELD|[-72.577769, 42.1...|22115|   MA|\n",
      "|01105|     SPRINGFIELD|[-72.578312, 42.0...|14970|   MA|\n",
      "|01106|      LONGMEADOW|[-72.5676, 42.050...|15688|   MA|\n",
      "|01107|     SPRINGFIELD|[-72.606544, 42.1...|12739|   MA|\n",
      "|01108|     SPRINGFIELD|[-72.558432, 42.0...|25519|   MA|\n",
      "+-----+----------------+--------------------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from zips where pop > 10000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips.createOrReplaceTempView(\"zips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misma consulta que antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-----+\n",
      "|         city|  pop|state|\n",
      "+-------------+-----+-----+\n",
      "|        ADELL| 1221|   WI|\n",
      "|     ALLENTON| 1449|   WI|\n",
      "|      BELGIUM| 2333|   WI|\n",
      "|   BROOKFIELD|19793|   WI|\n",
      "|  SOUTH BYRON| 1842|   WI|\n",
      "|       BUTLER| 2079|   WI|\n",
      "|CAMPBELLSPORT| 6057|   WI|\n",
      "|      CASCADE|  632|   WI|\n",
      "|    CEDARBURG|17552|   WI|\n",
      "|  CEDAR GROVE| 2656|   WI|\n",
      "|      CHILTON| 7495|   WI|\n",
      "|    CLEVELAND| 2490|   WI|\n",
      "|      COLGATE| 4155|   WI|\n",
      "|    DELAFIELD| 4837|   WI|\n",
      "|         EDEN| 1874|   WI|\n",
      "| ELKHART LAKE| 4665|   WI|\n",
      "|      WAUBEKA| 3942|   WI|\n",
      "|   GERMANTOWN|13053|   WI|\n",
      "|   GLENBEULAH| 1965|   WI|\n",
      "|      GRAFTON|12526|   WI|\n",
      "+-------------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select city, pop, state from zips where state = 'WI'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtén la única ciudad con más de 100 códigos postales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|count(_id)|   city|\n",
      "+----------+-------+\n",
      "|       101|HOUSTON|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select count(_id), city from zips group by city having count(_id) > 100\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtén la población del estado WINSCONSIN (WI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|sum(pop)|state|\n",
      "+--------+-----+\n",
      "| 4891769|   WI|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select sum(pop), state from zips where state = 'WI' group by state\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtén los 5 estados más poblados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|sum(pop)|state|\n",
      "+--------+-----+\n",
      "|29760021|   CA|\n",
      "|17990455|   NY|\n",
      "|16986510|   TX|\n",
      "|12937926|   FL|\n",
      "|11881643|   PA|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select sum(pop), state from zips group by state order by sum(pop) desc limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento iterativo con Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Find K Means of Loudacre device status locations\n",
    "# \n",
    "# Input data: file(s) with device status data (delimited by '|')\n",
    "# including latitude (13th field) and longitude (14th field) of device locations \n",
    "# (lat,lon of 0,0 indicates unknown location)\n",
    "#\n",
    "# Copy to pyspark using %paste\n",
    "\n",
    "import sys\n",
    "import operator\n",
    "\n",
    "# for a point p and an array of points, return the index in the array of the point closest to p\n",
    "def closestPoint(p, points):\n",
    "    bestIndex = 0\n",
    "    closest = float(\"+inf\")\n",
    "    # for each point in the array, calculate the distance to the test point, then return\n",
    "    # the index of the array point with the smallest distance\n",
    "    for i in range(len(points)):\n",
    "        dist = distanceSquared(p,points[i])\n",
    "        if dist < closest:\n",
    "            closest = dist\n",
    "            bestIndex = i\n",
    "    return bestIndex\n",
    "    \n",
    "# The squared distances between two points\n",
    "def distanceSquared(p1,p2):  \n",
    "    return (p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2\n",
    "\n",
    "# The sum of two points\n",
    "def addPoints(p1,p2):\n",
    "    return [p1[0] + p2[0], p1[1] + p2[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the file with device status data\n",
    "filename = \"/tmp/curso/devicestatus.txt\"\n",
    "    \n",
    "# K is the number of means (center points of clusters) to find\n",
    "K = 5\n",
    "\n",
    "# ConvergeDist -- the threshold \"distance\" between iterations at which we decide we are done\n",
    "convergeDist = .1\n",
    "        \n",
    "# Split by delimiter |\n",
    "# Parse  latitude and longitude (13th and 14th fields) into two-element arrays\n",
    "# Filter out records where lat/long is unavailable -- ie: 0/0 points\n",
    "points = sc.textFile(filename)\\\n",
    "     .map(lambda line: line.split(\"|\"))\\\n",
    "     .map(lambda fields: [float(fields[12]),float(fields[13])])\\\n",
    "     .filter(lambda point: sum(point) != 0)\\\n",
    "     .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[33.6894754264, -117.543308253],\n",
       " [37.4321088904, -121.485029632],\n",
       " [39.4378908349, -120.938978486],\n",
       " [39.3635186767, -119.400334708],\n",
       " [33.1913581092, -116.448242643],\n",
       " [33.8343543748, -117.330000857],\n",
       " [37.3803954321, -121.840756755],\n",
       " [34.1841062345, -117.9435329],\n",
       " [32.2850556785, -111.819583734],\n",
       " [45.2400522984, -122.377467861]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting points: [[38.0693533319, -119.880357402], [34.4625898526, -117.911415397], [35.1451661313, -114.391437632], [34.5761731945, -118.333818197], [33.1112265424, -116.688116073]]\n"
     ]
    }
   ],
   "source": [
    "# start with K randomly selected points from the dataset\n",
    "kPoints = points.takeSample(False, K, 34)\n",
    "print(\"Starting points:\", kPoints )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tempDist = float(\"+inf\")\n",
    "#while tempDist > convergeDist:\n",
    "\n",
    "closest = points.map(lambda p : (closestPoint(p, kPoints), (p, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ([33.6894754264, -117.543308253], 1)),\n",
       " (0, ([37.4321088904, -121.485029632], 1)),\n",
       " (0, ([39.4378908349, -120.938978486], 1)),\n",
       " (0, ([39.3635186767, -119.400334708], 1)),\n",
       " (4, ([33.1913581092, -116.448242643], 1)),\n",
       " (1, ([33.8343543748, -117.330000857], 1)),\n",
       " (0, ([37.3803954321, -121.840756755], 1)),\n",
       " (1, ([34.1841062345, -117.9435329], 1)),\n",
       " (2, ([32.2850556785, -111.819583734], 1)),\n",
       " (0, ([45.2400522984, -122.377467861], 1))]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointStats = closest.reduceByKey(lambda x,y:  (addPoints(x[0],y[0]),x[1]+y[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ([7759848.397078631, -23746914.124801286], 195852)),\n",
       " (4, ([919204.2197232544, -3203737.389424924], 27471)),\n",
       " (2, ([2459754.6515606306, -7906470.1069608815], 70108)),\n",
       " (1, ([3182439.2769901217, -10940250.653397888], 92878)),\n",
       " (3, ([1588915.261075296, -5417741.60234502], 45548))]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointStats.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPoints = pointStats.map(lambda x: (x[0],[x[1][0][0]/x[1][1],x[1][0][1]/x[1][1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [39.62098113411469, -121.24928070584566]),\n",
       " (4, [33.460894023634175, -116.62252518746764]),\n",
       " (2, [35.085220681814214, -112.77557635306786]),\n",
       " (1, [34.26472659822694, -117.7916261482578]),\n",
       " (3, [34.884413389727236, -118.94576276334898])]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newPoints.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[53] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between iterations: 7.545657991564262\n",
      "Distance between iterations: 1.5235590923308964\n",
      "Distance between iterations: 0.6441396643162632\n",
      "Distance between iterations: 1.175595739057369\n",
      "Distance between iterations: 2.196319966517338\n",
      "Distance between iterations: 2.5070402295136542\n",
      "Distance between iterations: 0.12556359696183592\n",
      "Distance between iterations: 0.03493171059116467\n",
      "Final center points: [[43.97544955558915, -121.40498164360471], [34.49209866867575, -118.21258003843369], [35.0852504610273, -112.57489358662798], [38.17337679610569, -121.21451034445965], [33.76035813785657, -116.56967965470706]]\n"
     ]
    }
   ],
   "source": [
    "# loop until the total distance between one iteration's points and the next is less than the convergence distance specified\n",
    "tempDist = float(\"+inf\")\n",
    "while tempDist > convergeDist:\n",
    "    # for each point, find the index of the closest kpoint.  map to (index, (point,1))\n",
    "    closest = points.map(lambda p : (closestPoint(p, kPoints), (p, 1)))\n",
    "    # For each key (k-point index), reduce by adding the coordinates and number of points\n",
    "    #pointStats = closest.reduceByKey(lambda (point1,n1),(point2,n2):  (addPoints(point1,point2),n1+n2) )\n",
    "    pointStats = closest.reduceByKey(lambda x,y:  (addPoints(x[0],y[0]),x[1]+y[1]) )\n",
    "    # For each key (k-point index), find a new point by calculating the average of each closest point\n",
    "    #newPoints = pointStats.map(lambda (i,(point,n)): (i,[point[0]/n,point[1]/n])).collect()\n",
    "    newPoints = pointStats.map(lambda x: (x[0],[x[1][0][0]/x[1][1],x[1][0][1]/x[1][1]]))\n",
    "    # calculate the total of the distance between the current points and new points\n",
    "    tempDist=0\n",
    "    for  (i,point) in newPoints.collect(): \n",
    "        tempDist += distanceSquared(kPoints[i],point)\n",
    "    print(\"Distance between iterations:\",tempDist)\n",
    "    # Copy the new points to the kPoints array for the next iteration\n",
    "    for (i, point) in newPoints.collect(): \n",
    "        kPoints[i] = point\n",
    "        \n",
    "print(\"Final center points: \" + str(kPoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stoping Spark"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
